{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69385bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86d52172",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdevanshi\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSHL-Assessment-Recommendation-System_Devanshi-Singh\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_recommender\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMRecommender\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfaiss_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FaissIndex\n",
      "File \u001b[1;32mc:\\Users\\devanshi\\SHL-Assessment-Recommendation-System_Devanshi-Singh\\src\\evaluation\\..\\llm\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mLLM integration package for SHL Assessment Recommender.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemini_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeminiClient\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_recommender\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMRecommender\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeminiClient\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLMRecommender\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\devanshi\\SHL-Assessment-Recommendation-System_Devanshi-Singh\\src\\evaluation\\..\\llm\\llm_recommender.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemini_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeminiClient\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfaiss_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FaissIndex  \u001b[38;5;66;03m# Import your vector search class\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLLMRecommender\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    LLM-enhanced recommender system that combines vector search with LLM reranking.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "sys.path.append(r\"C:\\Users\\devanshi\\SHL-Assessment-Recommendation-System_Devanshi-Singh\\src\")\n",
    "\n",
    "from llm.llm_recommender import LLMRecommender\n",
    "from embeddings.faiss_wrapper import FaissIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f133e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45 test samples\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print(f\"Loaded {len(test_df)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e500b7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 unique queries\n"
     ]
    }
   ],
   "source": [
    "# Group ground truth by query\n",
    "ground_truth = {}\n",
    "for _, row in test_df.iterrows():\n",
    "    query = row['Query']\n",
    "    assessment = row['Assessments']\n",
    "    if query not in ground_truth:\n",
    "        ground_truth[query] = []\n",
    "    ground_truth[query].append(assessment)\n",
    "\n",
    "print(f\"Found {len(ground_truth)} unique queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c7317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "def calculate_metrics(relevant: List[str], recommended: List[str], k_values: List[int]):\n",
    "    results = {\"recall\": {}, \"ap\": {}}\n",
    "    \n",
    "    for k in k_values:\n",
    "        # Calculate Recall@K\n",
    "        recommended_k = recommended[:k]\n",
    "        relevant_found = set(relevant).intersection(set(recommended_k))\n",
    "        recall = len(relevant_found) / len(relevant) if relevant else 0\n",
    "        \n",
    "        # Calculate AP@K\n",
    "        ap = 0.0\n",
    "        hits = 0\n",
    "        for i, item in enumerate(recommended_k):\n",
    "            if item in relevant:\n",
    "                hits += 1\n",
    "                ap += hits / (i + 1)\n",
    "        ap = ap / min(k, len(relevant)) if min(k, len(relevant)) > 0 else 0\n",
    "        \n",
    "        results[\"recall\"][k] = recall\n",
    "        results[\"ap\"][k] = ap\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0573ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize recommender\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"Warning: GEMINI_API_KEY not found\")\n",
    "    api_key = input(\"Enter your Gemini API key: \")\n",
    "\n",
    "try:\n",
    "    index_path = input(\"Enter path to FAISS index: \")\n",
    "    vector_index = FaissIndex.load(index_path)\n",
    "    recommender = LLMRecommender(vector_index=vector_index, api_key=api_key)\n",
    "    print(\"Recommender initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation parameters\n",
    "k_values = [1, 3, 5, 10]\n",
    "metrics = {\"recall\": {k: [] for k in k_values}, \"ap\": {k: [] for k in k_values}}\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\nEvaluating recommender system...\")\n",
    "for query, relevant in tqdm(ground_truth.items()):\n",
    "    try:\n",
    "        # Get recommendations\n",
    "        recommendations = recommender.recommend(\n",
    "            job_description=query,\n",
    "            top_k=20,\n",
    "            rerank=True,\n",
    "            final_results=max(k_values)\n",
    "        )\n",
    "        \n",
    "        # Extract assessment names\n",
    "        recommended_names = [rec.get(\"name\", \"\") for rec in recommendations]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        query_metrics = calculate_metrics(relevant, recommended_names, k_values)\n",
    "        \n",
    "        # Store results\n",
    "        for k in k_values:\n",
    "            metrics[\"recall\"][k].append(query_metrics[\"recall\"][k])\n",
    "            metrics[\"ap\"][k].append(query_metrics[\"ap\"][k])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query '{query[:30]}...': {e}\")\n",
    "        for k in k_values:\n",
    "            metrics[\"recall\"][k].append(0.0)\n",
    "            metrics[\"ap\"][k].append(0.0)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_metrics = {\n",
    "    \"Mean Recall\": {k: np.mean(metrics[\"recall\"][k]) for k in k_values},\n",
    "    \"MAP\": {k: np.mean(metrics[\"ap\"][k]) for k in k_values}\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "print(f\"{'Metric':<15} | {'Value':<10} | {'K Value':<7}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"{'Mean Recall':<15} | {mean_metrics['Mean Recall'][k]:.4f}    | {k:<7}\")\n",
    "    print(f\"{'MAP':<15} | {mean_metrics['MAP'][k]:.4f}    | {k:<7}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    \"mean_metrics\": mean_metrics,\n",
    "    \"queries_evaluated\": len(ground_truth),\n",
    "    \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"Results saved to evaluation_results.json\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nEVALUATION SUMMARY:\")\n",
    "print(f\"- System achieved Mean Recall@3 of {mean_metrics['Mean Recall'][3]:.4f} ({mean_metrics['Mean Recall'][3]*100:.1f}%)\")\n",
    "print(f\"- MAP@3 score is {mean_metrics['MAP'][3]:.4f}\")\n",
    "print(f\"- Vector search with LLM reranking shows {'+' if mean_metrics['MAP'][3] > 0.5 else '-'}performance\")\n",
    "print(f\"- Best performing K value: {max(k_values, key=lambda k: mean_metrics['MAP'][k])}\")\n",
    "\n",
    "# Key observations\n",
    "if mean_metrics['Mean Recall'][3] >= 0.6:\n",
    "    print(\"+ Strong recall performance indicates good coverage of relevant assessments\")\n",
    "else:\n",
    "    print(\"- Lower recall suggests need for improved candidate selection\")\n",
    "    \n",
    "if mean_metrics['MAP'][3] >= 0.5:\n",
    "    print(\"+ Good MAP scores show effective ranking of relevant assessments\")\n",
    "else:\n",
    "    print(\"- MAP scores indicate room for improvement in reranking quality\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
